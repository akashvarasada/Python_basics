{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I enjoy coding with Python and machine learning\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to get the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the TF-IDF matrix (as dense array)\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Show feature names (words)\n",
    "print(\"\\nFeature Names (Words):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# Disable SSL verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Then try downloading\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello! How are you doing today?\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Subword Tokenization with BPE (Byte Pair Encoding)\n",
    "#You can use libraries like sentencepiece or tokenizers for subword tokenization. Hereâ€™s an example using tokenizers from Hugging Face:\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# Initialize tokenizer and trainer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(vocab_size=1000, min_frequency=2)\n",
    "\n",
    "# Sample sentences for training\n",
    "corpus = [\"Hello, how are you?\", \"I am learning NLP.\", \"NLP is fun.\"]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# Encode a text sample\n",
    "output = tokenizer.encode(\"Hello, how are you?\")\n",
    "print(\"Encoded Output:\", output.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"low frequency data\",\n",
    "    \"high frequency data\",\n",
    "    \"more data with high frequency\"\n",
    "]\n",
    "\n",
    "# Define a function to train BPE tokenizer with different min_frequency values\n",
    "def train_bpe(min_frequency):\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    trainer = BpeTrainer(vocab_size=50, min_frequency=min_frequency)\n",
    "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Train tokenizer with min_frequency=2\n",
    "tokenizer_2 = train_bpe(min_frequency=2)\n",
    "\n",
    "# Train tokenizer with min_frequency=1 (allowing all pairs)\n",
    "tokenizer_1 = train_bpe(min_frequency=1)\n",
    "\n",
    "# Check vocabulary for both models\n",
    "vocab_2 = tokenizer_2.get_vocab()\n",
    "vocab_1 = tokenizer_1.get_vocab()\n",
    "\n",
    "print(\"Vocabulary with min_frequency=2:\", list(vocab_2.keys())[:])  # Show the first 10 tokens\n",
    "print(\"Vocabulary with min_frequency=1:\", list(vocab_1.keys())[:])  # Show the first 10 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Word Embeddings\n",
    "# Word embeddings are a type of word representation that allows words to be represented as dense vectors in a continuous vector space. Common algorithms for generating word embeddings include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "# Example: Using Pre-trained Word Embeddings (GloVe) with Gensim\n",
    "# You can load pre-trained word embeddings (like GloVe) using the gensim library.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional GloVe vectors\n",
    "\n",
    "# Check similarity between words\n",
    "similarity = glove.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity}\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words = glove.most_similar('king', topn=5)\n",
    "print(\"Words similar to 'king':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    [\"hello\", \"how\", \"are\", \"you\"],\n",
    "    [\"I\", \"am\", \"learning\", \"NLP\"],\n",
    "    [\"NLP\", \"is\", \"fun\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find most similar words to \"NLP\"\n",
    "similar_words = model.wv.most_similar(\"NLP\", topn=5)\n",
    "print(\"Words similar to 'NLP':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Zero-Shot Learning\n",
    "# Zero-shot learning refers to the ability of a model to perform a task without having seen any examples of that specific task during training. It is a powerful feature for tasks like text classification, where the model can classify text into categories it hasn't been explicitly trained on. Models like GPT-3 and BERT can be used for zero-shot tasks via prompt engineering.\n",
    "\n",
    "# Example: Zero-Shot Text Classification using Hugging Face's transformers library\n",
    "# Hugging Face provides a zero-shot classification pipeline using models like BART and RoBERTa.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a zero-shot classification model\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I love playing soccer on the weekends.\"\n",
    "\n",
    "# Define candidate labels\n",
    "candidate_labels = [\"sports\", \"cooking\", \"politics\", \"technology\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(\"Zero-Shot Classification Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Specify a model explicitly\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "# Perform NER\n",
    "entities = ner_model(text)\n",
    "\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
