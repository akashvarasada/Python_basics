{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.         0.         0.         0.54645401\n",
      "  0.         0.         0.54645401 0.         0.54645401 0.32274454\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.4711101  0.4711101  0.\n",
      "  0.4711101  0.35829137 0.         0.35829137 0.         0.27824521\n",
      "  0.        ]\n",
      " [0.4261835  0.4261835  0.4261835  0.         0.         0.\n",
      "  0.         0.32412354 0.         0.32412354 0.         0.25171084\n",
      "  0.4261835 ]]\n",
      "\n",
      "Feature Names (Words):\n",
      "['and' 'coding' 'enjoy' 'for' 'great' 'in' 'is' 'learning' 'love'\n",
      " 'machine' 'programming' 'python' 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I enjoy coding with Python and machine learning\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to get the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the TF-IDF matrix (as dense array)\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Show feature names (words)\n",
    "print(\"\\nFeature Names (Words):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello! How are you doing today?\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Output: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'h', 'o', 'w', ' a', 'r', 'e', ' ', 'y', 'o', 'u', '?']\n"
     ]
    }
   ],
   "source": [
    "#Example: Subword Tokenization with BPE (Byte Pair Encoding)\n",
    "#You can use libraries like sentencepiece or tokenizers for subword tokenization. Hereâ€™s an example using tokenizers from Hugging Face:\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# Initialize tokenizer and trainer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(vocab_size=1000, min_frequency=2)\n",
    "\n",
    "# Sample sentences for training\n",
    "corpus = [\"Hello, how are you?\", \"I am learning NLP.\", \"NLP is fun.\"]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# Encode a text sample\n",
    "output = tokenizer.encode(\"Hello, how are you?\")\n",
    "print(\"Encoded Output:\", output.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with min_frequency=2: ['i', 'e', 'a', 'qu', 'cy', 'en', 'd', 'at', 'g', 'y', 'ency', 'n', 'w', 'hi', 'u', 'r', 'q', 'f', 't', 'c', ' ', 'h', 're', ' f', 'requ', ' dat', ' data', ' frequency data', ' frequency', 'l', 'm', 'gh', 'o', ' d', ' frequ', 'high']\n",
      "Vocabulary with min_frequency=1: [' f', 'at', 'u', 'y', 're data', 'n', ' frequ', 'qu', 'gh', ' frequency data', 'a', 'hi', 'ency', 'high frequency data', 'it', 'c', ' dat', 'requ', 'e', 'f', 'd', 'g', ' ', 'h', 't', 'o', 're', ' frequency', 'm', ' data', 'high', ' wit', 'h high frequency', 'cy', ' with high frequency', 'q', 'lo', 'i', 'en', 'mo', 'h high', 'more data', ' d', 'low frequency data', 'r', ' w', 'l', ' high', 'w frequency data', 'w']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"low frequency data\",\n",
    "    \"high frequency data\",\n",
    "    \"more data with high frequency\"\n",
    "]\n",
    "\n",
    "# Define a function to train BPE tokenizer with different min_frequency values\n",
    "def train_bpe(min_frequency):\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    trainer = BpeTrainer(vocab_size=50, min_frequency=min_frequency)\n",
    "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Train tokenizer with min_frequency=2\n",
    "tokenizer_2 = train_bpe(min_frequency=2)\n",
    "\n",
    "# Train tokenizer with min_frequency=1 (allowing all pairs)\n",
    "tokenizer_1 = train_bpe(min_frequency=1)\n",
    "\n",
    "# Check vocabulary for both models\n",
    "vocab_2 = tokenizer_2.get_vocab()\n",
    "vocab_1 = tokenizer_1.get_vocab()\n",
    "\n",
    "print(\"Vocabulary with min_frequency=2:\", list(vocab_2.keys())[:])  # Show the first 10 tokens\n",
    "print(\"Vocabulary with min_frequency=1:\", list(vocab_1.keys())[:])  # Show the first 10 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7507690787315369\n",
      "Words similar to 'king': [('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175)]\n"
     ]
    }
   ],
   "source": [
    "# 3. Word Embeddings\n",
    "# Word embeddings are a type of word representation that allows words to be represented as dense vectors in a continuous vector space. Common algorithms for generating word embeddings include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "# Example: Using Pre-trained Word Embeddings (GloVe) with Gensim\n",
    "# You can load pre-trained word embeddings (like GloVe) using the gensim library.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional GloVe vectors\n",
    "\n",
    "# Check similarity between words\n",
    "similarity = glove.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity}\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words = glove.most_similar('king', topn=5)\n",
    "print(\"Words similar to 'king':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'NLP': [('hello', 0.21617142856121063), ('are', 0.09291722625494003), ('how', 0.027057476341724396), ('you', 0.016134677454829216), ('fun', -0.01083916611969471)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    [\"hello\", \"how\", \"are\", \"you\"],\n",
    "    [\"I\", \"am\", \"learning\", \"NLP\"],\n",
    "    [\"NLP\", \"is\", \"fun\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find most similar words to \"NLP\"\n",
    "similar_words = model.wv.most_similar(\"NLP\", topn=5)\n",
    "print(\"Words similar to 'NLP':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification Result:\n",
      "{'sequence': 'I love playing soccer on the weekends.', 'labels': ['sports', 'technology', 'cooking', 'politics'], 'scores': [0.9956403970718384, 0.0020962031558156013, 0.0013117057969793677, 0.0009516139980405569]}\n"
     ]
    }
   ],
   "source": [
    "# 4. Zero-Shot Learning\n",
    "# Zero-shot learning refers to the ability of a model to perform a task without having seen any examples of that specific task during training. It is a powerful feature for tasks like text classification, where the model can classify text into categories it hasn't been explicitly trained on. Models like GPT-3 and BERT can be used for zero-shot tasks via prompt engineering.\n",
    "\n",
    "# Example: Zero-Shot Text Classification using Hugging Face's transformers library\n",
    "# Hugging Face provides a zero-shot classification pipeline using models like BART and RoBERTa.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a zero-shot classification model\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I love playing soccer on the weekends.\"\n",
    "\n",
    "# Define candidate labels\n",
    "candidate_labels = [\"sports\", \"cooking\", \"politics\", \"technology\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(\"Zero-Shot Classification Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19baf8286ec4c5990331e5c3a28421d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash.varasada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\akash.varasada\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4733c130c8d40cab80c3645d20d387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fcf2c2f3754659be3f31702a591235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [{'entity': 'I-PER', 'score': 0.9990103, 'index': 1, 'word': 'Barack', 'start': 0, 'end': 6}, {'entity': 'I-PER', 'score': 0.999342, 'index': 2, 'word': 'Obama', 'start': 7, 'end': 12}, {'entity': 'I-LOC', 'score': 0.99945, 'index': 6, 'word': 'Hawaii', 'start': 25, 'end': 31}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Specify a model explicitly\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "# Perform NER\n",
    "entities = ner_model(text)\n",
    "\n",
    "print(\"Named Entities:\", entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
